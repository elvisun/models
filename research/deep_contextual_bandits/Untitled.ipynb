{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from bandits.algorithms.bootstrapped_bnn_sampling import BootstrappedBNNSampling\n",
    "from bandits.core.contextual_bandit import run_contextual_bandit\n",
    "from bandits.data.data_sampler import sample_adult_data\n",
    "from bandits.data.data_sampler import sample_census_data\n",
    "from bandits.data.data_sampler import sample_covertype_data\n",
    "from bandits.data.data_sampler import sample_jester_data\n",
    "\n",
    "from bandits.data.data_sampler import sample_statlog_data\n",
    "from bandits.data.data_sampler import sample_stock_data\n",
    "from bandits.algorithms.fixed_policy_sampling import FixedPolicySampling\n",
    "from bandits.algorithms.linear_full_posterior_sampling import LinearFullPosteriorSampling\n",
    "from bandits.algorithms.neural_linear_sampling import NeuralLinearPosteriorSampling\n",
    "from bandits.algorithms.parameter_noise_sampling import ParameterNoiseSampling\n",
    "from bandits.algorithms.posterior_bnn_sampling import PosteriorBNNSampling\n",
    "from bandits.data.synthetic_data_sampler import sample_linear_data\n",
    "from bandits.data.synthetic_data_sampler import sample_sparse_linear_data\n",
    "from bandits.data.synthetic_data_sampler import sample_wheel_bandit_data\n",
    "from bandits.algorithms.uniform_sampling import UniformSampling\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your file routes to the data files.\n",
    "base_route = os.getcwd()\n",
    "data_route = 'contextual_bandits/datasets'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS.set_default('alsologtostderr', True)\n",
    "# flags.DEFINE_string('logdir', '/tmp/bandits/', 'Base directory to save output')\n",
    "# flags.DEFINE_string(\n",
    "#     'mushroom_data',\n",
    "#     os.path.join(base_route, data_route, 'mushroom.data'),\n",
    "#     'Directory where Mushroom data is stored.')\n",
    "# flags.DEFINE_string(\n",
    "#     'financial_data',\n",
    "#     os.path.join(base_route, data_route, 'raw_stock_contexts'),\n",
    "#     'Directory where Financial data is stored.')\n",
    "# flags.DEFINE_string(\n",
    "#     'jester_data',\n",
    "#     os.path.join(base_route, data_route, 'jester_data_40jokes_19181users.npy'),\n",
    "#     'Directory where Jester data is stored.')\n",
    "# flags.DEFINE_string(\n",
    "#     'statlog_data',\n",
    "#     os.path.join(base_route, data_route, 'shuttle.trn'),\n",
    "#     'Directory where Statlog data is stored.')\n",
    "# flags.DEFINE_string(\n",
    "#     'adult_data',\n",
    "#     os.path.join(base_route, data_route, 'adult.full'),\n",
    "#     'Directory where Adult data is stored.')\n",
    "# flags.DEFINE_string(\n",
    "#     'covertype_data',\n",
    "#     os.path.join(base_route, data_route, 'covtype.data'),\n",
    "#     'Directory where Covertype data is stored.')\n",
    "# flags.DEFINE_string(\n",
    "#     'census_data',\n",
    "#     os.path.join(base_route, data_route, 'USCensus1990.data.txt'),\n",
    "#     'Directory where Census data is stored.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_hot(df, cols):\n",
    "  \"\"\"Returns one-hot encoding of DataFrame df including columns in cols.\"\"\"\n",
    "  for col in cols:\n",
    "    dummies = pd.get_dummies(df[col], prefix=col, drop_first=False)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df = df.drop(col, axis=1)\n",
    "  return df\n",
    "\n",
    "\n",
    "def sample_mushroom_data(file_name,\n",
    "                         num_contexts,\n",
    "                         r_noeat=0,\n",
    "                         r_eat_safe=5,\n",
    "                         r_eat_poison_bad=-35,\n",
    "                         r_eat_poison_good=5,\n",
    "                         prob_poison_bad=0.5):\n",
    "  \"\"\"Samples bandit game from Mushroom UCI Dataset.\n",
    "\n",
    "  Args:\n",
    "    file_name: Route of file containing the original Mushroom UCI dataset.\n",
    "    num_contexts: Number of points to sample, i.e. (context, action rewards).\n",
    "    r_noeat: Reward for not eating a mushroom.\n",
    "    r_eat_safe: Reward for eating a non-poisonous mushroom.\n",
    "    r_eat_poison_bad: Reward for eating a poisonous mushroom if harmed.\n",
    "    r_eat_poison_good: Reward for eating a poisonous mushroom if not harmed.\n",
    "    prob_poison_bad: Probability of being harmed by eating a poisonous mushroom.\n",
    "\n",
    "  Returns:\n",
    "    dataset: Sampled matrix with n rows: (context, eat_reward, no_eat_reward).\n",
    "    opt_vals: Vector of expected optimal (reward, action) for each context.\n",
    "\n",
    "  We assume r_eat_safe > r_noeat, and r_eat_poison_good > r_eat_poison_bad.\n",
    "  \"\"\"\n",
    "\n",
    "  # first two cols of df encode whether mushroom is edible or poisonous\n",
    "  df = pd.read_csv(file_name, header=None)\n",
    "  df = one_hot(df, df.columns)\n",
    "  ind = np.random.choice(range(df.shape[0]), num_contexts, replace=True)\n",
    "\n",
    "  contexts = df.iloc[ind, 2:]\n",
    "  no_eat_reward = r_noeat * np.ones((num_contexts, 1))\n",
    "  random_poison = np.random.choice(\n",
    "      [r_eat_poison_bad, r_eat_poison_good],\n",
    "      p=[prob_poison_bad, 1 - prob_poison_bad],\n",
    "      size=num_contexts)\n",
    "  eat_reward = r_eat_safe * df.iloc[ind, 0]\n",
    "  eat_reward += np.multiply(random_poison, df.iloc[ind, 1])\n",
    "  eat_reward = eat_reward.values.reshape((num_contexts, 1))\n",
    "\n",
    "  # compute optimal expected reward and optimal actions\n",
    "  exp_eat_poison_reward = r_eat_poison_bad * prob_poison_bad\n",
    "  exp_eat_poison_reward += r_eat_poison_good * (1 - prob_poison_bad)\n",
    "  opt_exp_reward = r_eat_safe * df.iloc[ind, 0] + max(\n",
    "      r_noeat, exp_eat_poison_reward) * df.iloc[ind, 1]\n",
    "\n",
    "  if r_noeat > exp_eat_poison_reward:\n",
    "    # actions: no eat = 0 ; eat = 1\n",
    "    opt_actions = df.iloc[ind, 0]  # indicator of edible\n",
    "  else:\n",
    "    # should always eat (higher expected reward)\n",
    "    opt_actions = np.ones((num_contexts, 1))\n",
    "\n",
    "  opt_vals = (opt_exp_reward.values, opt_actions.values)\n",
    "\n",
    "  return np.hstack((contexts, no_eat_reward, eat_reward)), opt_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(data_type, num_contexts=None):\n",
    "  \"\"\"Sample data from given 'data_type'.\n",
    "\n",
    "  Args:\n",
    "    data_type: Dataset from which to sample.\n",
    "    num_contexts: Number of contexts to sample.\n",
    "\n",
    "  Returns:\n",
    "    dataset: Sampled matrix with rows: (context, reward_1, ..., reward_num_act).\n",
    "    opt_rewards: Vector of expected optimal reward for each context.\n",
    "    opt_actions: Vector of optimal action for each context.\n",
    "    num_actions: Number of available actions.\n",
    "    context_dim: Dimension of each context.\n",
    "  \"\"\"\n",
    "\n",
    "  if data_type == 'linear':\n",
    "    # Create linear dataset\n",
    "    num_actions = 8\n",
    "    context_dim = 10\n",
    "    noise_stds = [0.01 * (i + 1) for i in range(num_actions)]\n",
    "    dataset, _, opt_linear = sample_linear_data(num_contexts, context_dim,\n",
    "                                                num_actions, sigma=noise_stds)\n",
    "    opt_rewards, opt_actions = opt_linear\n",
    "  elif data_type == 'sparse_linear':\n",
    "    # Create sparse linear dataset\n",
    "    num_actions = 7\n",
    "    context_dim = 10\n",
    "    noise_stds = [0.01 * (i + 1) for i in range(num_actions)]\n",
    "    num_nnz_dims = int(context_dim / 3.0)\n",
    "    dataset, _, opt_sparse_linear = sample_sparse_linear_data(\n",
    "        num_contexts, context_dim, num_actions, num_nnz_dims, sigma=noise_stds)\n",
    "    opt_rewards, opt_actions = opt_sparse_linear\n",
    "  elif data_type == 'mushroom':\n",
    "    # Create mushroom dataset\n",
    "    num_actions = 2\n",
    "    context_dim = 117\n",
    "    file_name = 'contextual_bandits/datasets/mushroom.data'\n",
    "    dataset, opt_mushroom = sample_mushroom_data(file_name, num_contexts)\n",
    "    opt_rewards, opt_actions = opt_mushroom\n",
    "  elif data_type == 'financial':\n",
    "    num_actions = 8\n",
    "    context_dim = 21\n",
    "    num_contexts = min(3713, num_contexts)\n",
    "    noise_stds = [0.01 * (i + 1) for i in range(num_actions)]\n",
    "    file_name = FLAGS.financial_data\n",
    "    dataset, opt_financial = sample_stock_data(file_name, context_dim,\n",
    "                                               num_actions, num_contexts,\n",
    "                                               noise_stds, shuffle_rows=True)\n",
    "    opt_rewards, opt_actions = opt_financial\n",
    "  elif data_type == 'jester':\n",
    "    num_actions = 8\n",
    "    context_dim = 32\n",
    "    num_contexts = min(19181, num_contexts)\n",
    "    file_name = FLAGS.jester_data\n",
    "    dataset, opt_jester = sample_jester_data(file_name, context_dim,\n",
    "                                             num_actions, num_contexts,\n",
    "                                             shuffle_rows=True,\n",
    "                                             shuffle_cols=True)\n",
    "    opt_rewards, opt_actions = opt_jester\n",
    "  elif data_type == 'statlog':\n",
    "    file_name = FLAGS.statlog_data\n",
    "    num_actions = 7\n",
    "    num_contexts = min(43500, num_contexts)\n",
    "    sampled_vals = sample_statlog_data(file_name, num_contexts,\n",
    "                                       shuffle_rows=True)\n",
    "    contexts, rewards, (opt_rewards, opt_actions) = sampled_vals\n",
    "    dataset = np.hstack((contexts, rewards))\n",
    "    context_dim = contexts.shape[1]\n",
    "  elif data_type == 'adult':\n",
    "    file_name = FLAGS.adult_data\n",
    "    num_actions = 14\n",
    "    num_contexts = min(45222, num_contexts)\n",
    "    sampled_vals = sample_adult_data(file_name, num_contexts,\n",
    "                                     shuffle_rows=True)\n",
    "    contexts, rewards, (opt_rewards, opt_actions) = sampled_vals\n",
    "    dataset = np.hstack((contexts, rewards))\n",
    "    context_dim = contexts.shape[1]\n",
    "  elif data_type == 'covertype':\n",
    "    file_name = FLAGS.covertype_data\n",
    "    num_actions = 7\n",
    "    num_contexts = min(150000, num_contexts)\n",
    "    sampled_vals = sample_covertype_data(file_name, num_contexts,\n",
    "                                         shuffle_rows=True)\n",
    "    contexts, rewards, (opt_rewards, opt_actions) = sampled_vals\n",
    "    dataset = np.hstack((contexts, rewards))\n",
    "    context_dim = contexts.shape[1]\n",
    "  elif data_type == 'census':\n",
    "    file_name = FLAGS.census_data\n",
    "    num_actions = 9\n",
    "    num_contexts = min(150000, num_contexts)\n",
    "    sampled_vals = sample_census_data(file_name, num_contexts,\n",
    "                                      shuffle_rows=True)\n",
    "    contexts, rewards, (opt_rewards, opt_actions) = sampled_vals\n",
    "    dataset = np.hstack((contexts, rewards))\n",
    "    context_dim = contexts.shape[1]\n",
    "  elif data_type == 'wheel':\n",
    "    delta = 0.95\n",
    "    num_actions = 5\n",
    "    context_dim = 2\n",
    "    mean_v = [1.0, 1.0, 1.0, 1.0, 1.2]\n",
    "    std_v = [0.05, 0.05, 0.05, 0.05, 0.05]\n",
    "    mu_large = 50\n",
    "    std_large = 0.01\n",
    "    dataset, opt_wheel = sample_wheel_bandit_data(num_contexts, delta,\n",
    "                                                  mean_v, std_v,\n",
    "                                                  mu_large, std_large)\n",
    "    opt_rewards, opt_actions = opt_wheel\n",
    "\n",
    "  return dataset, opt_rewards, opt_actions, num_actions, context_dim\n",
    "\n",
    "\n",
    "def display_results(algos, opt_rewards, opt_actions, h_rewards, t_init, name):\n",
    "  \"\"\"Displays summary statistics of the performance of each algorithm.\"\"\"\n",
    "\n",
    "  print('---------------------------------------------------')\n",
    "  print('---------------------------------------------------')\n",
    "  print('{} bandit completed after {} seconds.'.format(\n",
    "    name, time.time() - t_init))\n",
    "  print('---------------------------------------------------')\n",
    "\n",
    "  performance_pairs = []\n",
    "  for j, a in enumerate(algos):\n",
    "    performance_pairs.append((a.name, np.sum(h_rewards[:, j])))\n",
    "  performance_pairs = sorted(performance_pairs,\n",
    "                             key=lambda elt: elt[1],\n",
    "                             reverse=True)\n",
    "  for i, (name, reward) in enumerate(performance_pairs):\n",
    "    print('{:3}) {:20}| \\t \\t total reward = {:10}.'.format(i, name, reward))\n",
    "\n",
    "  print('---------------------------------------------------')\n",
    "  print('Optimal total reward = {}.'.format(np.sum(opt_rewards)))\n",
    "  print('Frequency of optimal actions (action, frequency):')\n",
    "  print([[elt, list(opt_actions).count(elt)] for elt in set(opt_actions)])\n",
    "  print('---------------------------------------------------')\n",
    "  print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:101: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:06.436576 63272 module_wrapper.py:139] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:101: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:105: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.131717 63272 module_wrapper.py:139] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:105: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:108: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.138698 63272 module_wrapper.py:139] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:108: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:74: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.141691 63272 module_wrapper.py:139] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:74: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:74: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.142688 63272 module_wrapper.py:139] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:74: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tf_gpu_1\\lib\\site-packages\\tensorflow_core\\contrib\\layers\\python\\layers\\layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.144683 63272 deprecation.py:323] From D:\\anaconda\\envs\\tf_gpu_1\\lib\\site-packages\\tensorflow_core\\contrib\\layers\\python\\layers\\layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:83: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.158645 63272 deprecation.py:323] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:83: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:127: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.172635 63272 module_wrapper.py:139] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:127: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:132: The name tf.train.inverse_time_decay is deprecated. Please use tf.compat.v1.train.inverse_time_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.176596 63272 module_wrapper.py:139] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:132: The name tf.train.inverse_time_decay is deprecated. Please use tf.compat.v1.train.inverse_time_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.182581 63272 module_wrapper.py:139] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:193: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.186570 63272 module_wrapper.py:139] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:193: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:140: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0520 11:10:07.187567 63272 module_wrapper.py:139] From C:\\Users\\1\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py:140: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "ename": "UnparsedFlagAccessError",
     "evalue": "Trying to access flag --logdir before flags were parsed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnparsedFlagAccessError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-2a8a4a9611c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m   \u001b[0mUniformSampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Uniform Sampling'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m   \u001b[0mUniformSampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Uniform Sampling 2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m   \u001b[0mNeuralLinearPosteriorSampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NeuralLinear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams_nlinear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m   \u001b[0mNeuralLinearPosteriorSampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NeuralLinear2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams_nlinear2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m ]\n",
      "\u001b[1;32m~\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_linear_sampling.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, hparams, optimizer)\u001b[0m\n\u001b[0;32m     74\u001b[0m                                       \u001b[0mhparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                                       intercept=False)\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralBanditModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'{}-bnn'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, optimizer, hparams, name)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"verbose\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimes_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbuild_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\models\\research\\deep_contextual_bandits\\bandits\\algorithms\\neural_bandit_model.py\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_summaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         self.summary_writer = tf.summary.FileWriter(\n\u001b[1;32m--> 141\u001b[1;33m             \"{}/graph_{}\".format(FLAGS.logdir, self.name), self.sess.graph)\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mtvars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_gpu_1\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;31m# get too much noise.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnparsedFlagAccessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnparsedFlagAccessError\u001b[0m: Trying to access flag --logdir before flags were parsed."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Problem parameters\n",
    "num_contexts = 2000\n",
    "\n",
    "# Data type in {linear, sparse_linear, mushroom, financial, jester,\n",
    "#                 statlog, adult, covertype, census, wheel}\n",
    "data_type = 'mushroom'\n",
    "\n",
    "# Create dataset\n",
    "sampled_vals = sample_data(data_type, num_contexts)\n",
    "dataset, opt_rewards, opt_actions, num_actions, context_dim = sampled_vals\n",
    "\n",
    "# Define hyperparameters and algorithms\n",
    "hparams = tf.contrib.training.HParams(num_actions=num_actions)\n",
    "\n",
    "hparams_linear = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                           context_dim=context_dim,\n",
    "                                           a0=6,\n",
    "                                           b0=6,\n",
    "                                           lambda_prior=0.25,\n",
    "                                           initial_pulls=2)\n",
    "\n",
    "hparams_nlinear = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                            context_dim=context_dim,\n",
    "                                            init_scale=0.3,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            layer_sizes=[50],\n",
    "                                            batch_size=512,\n",
    "                                            activate_decay=True,\n",
    "                                            initial_lr=0.1,\n",
    "                                            max_grad_norm=5.0,\n",
    "                                            show_training=False,\n",
    "                                            freq_summary=1000,\n",
    "                                            buffer_s=-1,\n",
    "                                            initial_pulls=2,\n",
    "                                            reset_lr=True,\n",
    "                                            lr_decay_rate=0.5,\n",
    "                                            training_freq=1,\n",
    "                                            training_freq_network=50,\n",
    "                                            training_epochs=100,\n",
    "                                            a0=6,\n",
    "                                            b0=6,\n",
    "                                            lambda_prior=0.25)\n",
    "\n",
    "hparams_nlinear2 = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                             context_dim=context_dim,\n",
    "                                             init_scale=0.3,\n",
    "                                             activation=tf.nn.relu,\n",
    "                                             layer_sizes=[50],\n",
    "                                             batch_size=512,\n",
    "                                             activate_decay=True,\n",
    "                                             initial_lr=0.1,\n",
    "                                             max_grad_norm=5.0,\n",
    "                                             show_training=False,\n",
    "                                             freq_summary=1000,\n",
    "                                             buffer_s=-1,\n",
    "                                             initial_pulls=2,\n",
    "                                             reset_lr=True,\n",
    "                                             lr_decay_rate=0.5,\n",
    "                                             training_freq=10,\n",
    "                                             training_freq_network=50,\n",
    "                                             training_epochs=100,\n",
    "                                             a0=6,\n",
    "                                             b0=6,\n",
    "                                             lambda_prior=0.25)\n",
    "\n",
    "\n",
    "algos = [\n",
    "  UniformSampling('Uniform Sampling', hparams),\n",
    "  UniformSampling('Uniform Sampling 2', hparams),\n",
    "  NeuralLinearPosteriorSampling('NeuralLinear', hparams_nlinear),\n",
    "  NeuralLinearPosteriorSampling('NeuralLinear2', hparams_nlinear2),\n",
    "]\n",
    "\n",
    "# Run contextual bandit problem\n",
    "t_init = time.time()\n",
    "results = run_contextual_bandit(context_dim, num_actions, dataset, algos)\n",
    "_, h_rewards = results\n",
    "\n",
    "# Display results\n",
    "display_results(algos, opt_rewards, opt_actions, h_rewards, t_init, data_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnparsedFlagAccessError",
     "evalue": "Trying to access flag --mushroom_data before flags were parsed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnparsedFlagAccessError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1fba33462e73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-619505c57108>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[1;31m# Create dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m   \u001b[0msampled_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_contexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m   \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampled_vals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-3890847b9c81>\u001b[0m in \u001b[0;36msample_data\u001b[1;34m(data_type, num_contexts)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mnum_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mcontext_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m117\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmushroom_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_mushroom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_mushroom_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_contexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mopt_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt_mushroom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tf_gpu_1\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;31m# get too much noise.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnparsedFlagAccessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnparsedFlagAccessError\u001b[0m: Trying to access flag --mushroom_data before flags were parsed."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
